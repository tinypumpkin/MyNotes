# 转换算子
## value类型
```scala
//初始化spark
val conf = new SparkConf().setAppName("a1").setMaster("local[*]")
val sc = new SparkContext(conf)
//新建list
val li = List(1, 2, 3, 4 , 5, 6, 7)
val rdd1 = sc.makeRDD(li)
```
> map()映射
+ 一次处理一个元素
```scala
val r1 = rdd1.map(_ * 2)
```
> mapPartitions()以分区为单位执行Map
+ 以分区为单位作映射，适用于批处理（占内存）
```scala
val r2 = rdd1.mapPartitions(datas => {
    datas.map(_*2)
})
```
> mapPartitionsWithIndex() 分区索引映射--带分区号
+ 结构上类似mapPartitions但是多了个index表示分区号
```scala
val r3 = rdd1.mapPartitionsWithIndex((index, datas) => {   //传入2个参数int，和迭代器所以data要重新映射
    datas.map((index, _))
})
```
+ 利用模式匹配进行分区索引映射
```scala
val r3_1 = rdd1.mapPartitionsWithIndex((index, datas) => {
    index match {
    case 2 => datas.map(_ * 2)
    case _ => datas
    }
})
```
>glom()分区的每个元素转换数组
+ 将RDD中每个分区都变成数组，数组中元素类型与原分区元素类型一致
```scala
val r4 = rdd1.glom()
```
+ 验证glom转化后分区中元素位置
```scala
println("-------未经glom处理---")
val li = List(1, 2, 3, 4 , 5, 6, 7)
val r4 = sc.makeRDD(li, 3)
//glom方法1
r4.mapPartitionsWithIndex {
    (index, datas) =>
    println(s"${index}--->${datas.mkString(",")}")
    datas
}.collect()
//glom 处理
val r4_1 = r4.glom()
println("-------经过glom处理-----")
//glom方法2
r4_1.mapPartitionsWithIndex{
    //glom将分区的单个元素转换数组,获取
    (index,datas)=>{
    println(s"${index}--->${datas.next().mkString(",")}")
    datas
    }
}.collect()
```
>flatmap 扁平化映射
+ 括号内必须是嵌套集合
+ 将传入rdd中的每一个元素拆出放到新的rdd中做为返回值输出
```scala
val rdd=sc.makeRDD(List(List(1,2,4),List(5,6,8),),2)
rdd.flatmap(e=>e)
```
>group by 分组
```scala
//按偶数分组
val r5 = rdd1.groupBy(_ % 2)
```
+ 查看分组后分区情况
```scala
r5.mapPartitionsWithIndex(
    (index,datas)=>{
    println(index+"--->"+datas.mkString(","))
    datas
    }
).collect().foreach(println)
```
>filter()过滤,括号内为过滤条件为布尔类型
+ 初始条件
```scala
val li = List("石原里美","木田彩水","新恒结衣","桃乃木香奈","木田彩水","桃谷绘里香")
val rdd = sc.makeRDD(li)
```
+ 过滤语句
```scala
//过滤含有“香”的字符元素
val fi = rdd.filter(_.contains("香"))
fi.collect().foreach(println)
//此处过滤奇数 这里的li是开头定义的
val odd = li.filter(_ % 2 != 0)
odd.collect().foreach(println)
```
>采样--sample
+ withReplacement -- 是否抽样放回 true/false
+ fraction 抽样因子 
1. fraction -->在withReplacement=false情况下:取值[0,1]选择每个元素的概率    
2. fraction -->在withReplacement=true情况下:选择每个元素的期望次数,取值大于等于0
+ seed 随机种子（不常用）
```scala
val rdd = sc.makeRDD(1 to 10)

val pas1 = rdd9.sample(false, 0.3)  //fraction->每个元素的概率
val pas2 = rdd9.sample(true, 2)  //fraction->每个元素期望取出的次数
```
>distinct--对RDD元素去重
```scala
val li = List(1, 1, 2, 3, 4, 6, 5, 6, 7, 2, 8, 3, 9)
val dis = sc.makeRDD(li,5)
//去重
dis.distinct()
```
+ 查看去重前后分区变化
```scala
println("======去重前========")
dis.mapPartitionsWithIndex{
    (index,num)=>{
    println(s"${index}->${num.mkString(",")}")
    num
    }
}.collect()
//去重
val dis = c_dis.distinct()
println("======去重后========")
dis.mapPartitionsWithIndex{
    (index,num)=>{
    println(s"${index}->${num.mkString(",")}")
    num
    }
}.collect()
```
>重新分区--coalesce/repartition

+  coalesce 缩减分区  注意:coalesce默认对扩大分区不起作用(shuffle=false)
+  repartition 扩大分区
+  repartition底层调用coalesce默认执行shuffle，一般用于扩大分区
+  初始化
```scala
val li = List(1, 1, 2, 3, 4, 6, 5, 6, 7, 2, 8, 3, 9)
val rdd = sc.makeRDD(cli,5)
```
+ 缩减分区
```scala
//缩减分区
val coa = rdd.coalesce(2)
//查看缩减后分区变化
coa.mapPartitionsWithIndex{
    (index,datas)=>{println(s"${index}--->${datas.mkString(",")}")}
    datas
}.collect().foreach(println)
```
+ 扩大分区
```scala
//扩大分区
val rep = rdd.repartition(7)
//查看扩大后分区变化
rep.mapPartitionsWithIndex{
    (index,datas)=>{println(s"${index}--->${datas.mkString(",")}")}
    datas
}.collect().foreach(println)
```
>sort by 排序算子
```scala
val sort1 = rdd10.sortBy(_.toInt)
sort1.collect().foreach(println)
```
## 双value类型
+ 初始化
```scala
val li1 = List(1, 2, 3, 4 , 5, 6, 7)
val li2 = List(1, 2, 3, 4, 5)
val li3 = List(2, 5, 6, 7, 9)
val rdd1: RDD[Int] = sc.makeRDD(li1)
val rdd2: RDD[Int] = sc.makeRDD(li2)
val rdd3: RDD[Int] = sc.makeRDD(li3)
```
>并集 -- union
```scala
 rdd1.union(rdd2)
```
>交集 -- intersection
```scala
rdd1.intersection(rdd2)
```
>差集 --subtract
```scala
rdd1.subtract(rdd2)
```
>拉链 -- zip 把两个RDD中的元素以（k-v）键值对形式组成为1个

+  元素个数不同，不能拉链
+  分区数不同，不能拉链
```scala
rdd2.zip(rdd2)
```

### WordCount案例
#### 简单版本
+ 方法1 --通过给字符串赋编号间接计算
```scala
println("========wordcount方法1===========")
val li = List("石原里美", "木田彩水", "新恒结衣", "桃乃木香奈", "木田彩水")
val rdd = sc.makeRDD(li)
val nrdd = rdd.flatMap(_.split(","))   //扁平映射
val cst = nrdd.map((_, 1))   //更改结构
val grdd = cst.groupBy(_._1)   //按key分组
val wc1 = grdd.map {
    case (k, v) => {
    (k, v.size)
    }
}
wc1.collect().foreach(println)
```
+ 方法2 --通过直接分组调方法直接计算
```scala
println("========wordcount方法2===========")
val grdd2 = fmrdd.groupBy(e => e)
val cw2 = grdd2.map(e => s"${e._1}-->${e._2.size}")
cw2.foreach(println)
```
#### 复杂版本
+ 初始化条件
```scala
val li = List(("石原里美 木田彩水 新恒结衣",5),( "桃乃木香奈 三上悠亚", 2),("木田彩水 桃谷绘里香",3))
val rdd = sc.makeRDD(li)
```
+ 方法3 --通过map转化结构让字符串乘其个数再扁平化映射
```scala
println("========wordcount方法3=========")
val rdd = rdd.map{
    case (str,num)=>{(str+" ")*num}
}.collect()
```
+ 方法4 --元组灵活利用模式匹配
```scala
println("========wordcount方法4=========")
val wcs = rdd.flatMap {
    case (words, count) => {
    words.split(" ").map(words => (words, count))
    }
}
//转化成元组在累加元组的datas里的数组元素
val wc3 = wcs.groupBy(key => key._1).map {
//      t=>{(t._1,t._2.map(d=>d._2).sum)}   //还原格式
    case (words, datas) => {
    s"${words}-->${datas.map(_._2).sum}"
    }
}
wc2.collect().foreach(println)
```

## key-value类型
>partation by 按照k重新分区
+ 将RDD[k,v]中的k按照partitioner重新分区
+  传入参数为分区器，partitioner默认hashpartitioner
+ 若原有partitionRDD与现有partitionRDD一致则不进行分区否则shuffle
+ 注意：RDD本身没有PartitionBy这个算子,通过隐式转换动态给k-v类型RDD拓展的功能
```scala
val rdd = sc.makeRDD(List((1,"桥本环奈"),(2,"木田彩水"), (3,"新恒结衣")),2)
//默认分区前
val rdd1 = rdd.partitionBy(new HashPartitioner(3))
//自定义分区器
val mp = rdd1.partitionBy(new Mypartationer(2))

//分区前
rdd1.mapPartitionsWithIndex{
(k,v)=>{println(s"${k}-->${v.mkString(",")}")
v
}
}.collect().foreach(println)
//分区后
mp.mapPartitionsWithIndex{
(k,v)=>{println(s"${k}-->${v.mkString(",")}")
v
}
}.collect().foreach(println)
sc.stop()
}}
```
+ 自定义分区器
```scala
class Mypartationer(partitions: Int) extends Partitioner{
//获取分区个数
override def numPartitions: Int =partitions
//指定分区规则--返回Int表示分区编号 (从0开始)
override def getPartition(key: Any): Int = {
if (key.isInstanceOf[String])
println("传入类型是字符串")
//编号对应分区放入,大于分区数的对不上往0仍
val bool = key.isInstanceOf[Int]
if (bool){
val i = key.asInstanceOf[Int]
if (i<partitions)
return i
0}
else 0
}}
```
>ReduceByKey 按key聚合
+ 将RDD[k,v]中元素按照相同的k对v进行聚合
+ 首先分区内聚合（combiner）->然后落盘分区间聚合（reduceByKey）
##### 利用ReduceByKey进行wordcount
```scala
val rdd = sc.makeRDD(List(("a", 1), ("b", 3), ("b", 2), ("a", 5)),2)
val rd = rdd.reduceByKey(_ + _) //聚合key，指定value运算规则

rd.mapPartitionsWithIndex{
(idnex,datas)=>{
println(s"${idnex}-->${datas.mkString(",")}")
datas}
}.collect()
```
>groupByKey 根据key对RDD分组
##### 利用groupByKey进行wordcount
```scala
val rdd = sc.makeRDD(List(("a", 1), ("b", 3), ("b", 2), ("a", 5)))
//可指定分区器，默认哈希
val gbk = rdd.groupByKey()
//遍历
var gmp = gbk.map {
    case (k, v) => {
    (k, v.sum)}
    }
gmp.collect().foreach(println)
```
+ ReduceByKey和groupByKey的区别
1. reduce 按key聚合shuffle前有combine（预聚合）不影响业务逻辑情况下使用  --聚合运算
2. group 按照key分组直接shuffle      --分组运算
>sortByKey 按Key排序,默认升序 
+ 按照key对RDD中元素排序
+ 
##### 对普通Seq进行排序
```scala
val rdd:RDD[(Int,String)] = sc.makeRDD(List((23,"桥本环奈"),(35,"木田彩水"), (32,"新恒结衣"), (37,"石原里美"), (24,"桃乃木")),2)

rdd.sortByKey(true)
```
##### 对自定义类排序要求混入特质
+ 自定义类
1. Ordered
2. Serializable
```scala
class student(var name:String,var age:Int) extends Ordered[student] with Serializable {
  //指定比较规则
  override def compare(that: student): Int = {
    //先按照名称排序，名称相同则按年龄
    var res = this.name.compareTo(that.name)
    if (res==0){
      res=this.age.compareTo(that.age)
    }
    res
  }
  override def toString = s"student($name, $age)"
}
```
```scala
val std = List((new student("桥本环奈", 23),1), (new student("新恒结衣", 32),1), (new student("桥本环奈", 24),1), (new student("木田彩水", 37),1))
val st = sc.makeRDD(std)
//对自定义类型进行排序
val sort = st.sortByKey()
```
>aggreateByKey
+ 按照key对分区内,分区间数据进行处理
+ zeroValue(初始值)-->给每个分区的每个key一个初始值
+ seqOp(分区内)-->用初始值迭代比较val
+ combOp(分区间)-->合并分区内容
+ 使用：aggregateByKey(初始值)(分区内计算规则,分区间计算规则)
##### 指定rdd
```scala
val rdd = sc.makeRDD(List(("a", 1), ("b", 3), ("b", 2), ("a", 5)),2)
```
##### 实现wordcount
```scala
//实现wordcount
val r:RDD[(String,Int)] = rdd.reduceByKey(_ + _)
rdd.aggregateByKey(0)(
    (x,y)=>x+y,   //参数1分区内计算规则
    (a,b)=>a+b    //参数2分区间计算规则
)
```
##### 分区最大值求和
```scala
val cal1 = rdd.aggregateByKey(0)(
    (x, y) => math.max(x, y), //参数1分区内计算规则
    (a, b) => a + b //参数2分区间计算规则
)
```
>foldByKey
+ AggreateByKey的简化版本
+ zeroValue --初始值
+ func 一个函数，两个输入参数相同（-->分区内分区间逻辑相同）
##### 利用foldByKey做wordcount
```scala
val rdd = sc.makeRDD(List(("a", 1), ("b", 3), ("b", 2), ("a", 5)),2)

val fbk = rdd.foldByKey(0)(_ + _)
```
#### reducebykey--foldbykeyu--aggregatebykeyu的对比（WordCount）
```scala
val rdd = sc.makeRDD(List(("a", 1), ("b", 3), ("b", 2), ("a", 5)),2)
```
+ reducebykey -->分区内分区间规则一样不需要指定初始值
```scala
val rbk = rdd.reduceByKey(_ + _)
```
+ foldbykeyu  -->分区内分区间规则一样要指定初始值
```scala
val fbk = rdd.foldByKey(0)(_ + _)
```
+ aggregatebykeyu  -->分区内分区间规则不一样要指定初始值
```scala
val abk = rdd.aggregateByKey(0)(_ + _,_+_)
```
>combineByKey
+ createCombiner：V=>C 初始化V读入的值做结构转化
+ mergeValue:（C,V）=>c 分区内计算规则 将分区val合并到初始化所得c上（val是元数据得到的值v）
+ mergeCombiner:（C,V）=>c分区间合并不同分区的元素
##### 要求：求平均分
##### 初始化RDD
```scala
val rdd:RDD[(String,Int)] = sc.makeRDD(List(("桥本环奈", 85),("木田彩水",75), ("新恒结衣", 75), ("木田彩水", 92), ("桃乃木", 35)),2)
```
##### 方案1 通过grouobykey(单大压力大)
```scala
val group1 = g1.map {
    case (name, gread) => (name, gread.sum / gread.size)
}
```
##### 方案2 通过reducebykey
```scala
//先转换结构
val r1 = rdd.map {
    case (n, g) => (n, (g, 1))
}
val rtemp = r1.reduceByKey {
    (d1, d2) => (d1._1 + d2._1, d1._2 + d2._2) //指定reduce聚合方法
}
val rbk = rtemp.map {
    case (a, (b, c)) => (a, b.toDouble / c)
}
```
##### <font color=blue>方案3：通过combineByKey算子</font>
```scala
val com = rdd.combineByKey(
(_, 1), //初始化rdd key 对key做结构化转换   ("桥本环奈", 85)-->("桥本环奈",(85,1))
(tup1: (Int, Int), v) => (tup1._1 + v, tup1._2 + 1), //分区内   木田彩水:(75,1) ->(75+92,1+1)=(167,2)
(tup2: (Int, Int), tup3: (Int, Int)) => (tup2._1 + tup3._1, tup2._2 + tup3._2)  //分区间合并不同分区的(成绩,个数)
)
val avg2 = com.map {
    case (name, tup) => println(s"${name}-->${tup._1 / tup._2}")
}
```

>mapValues --仅对value操作
 + 仅对values进行操作
```scala
val rdd = sc.makeRDD(List((23,"桥本环奈"),(35,"木田彩水"), (32,"新恒结衣"), (37,"木田彩水"), (24,"桃乃木")),2)

val v1 = rdd.mapValues("|||" + _)
v1.collect().foreach(println)

val v2 = rdd.mapValues(_ + "-->小姐姐")
v2.collect().foreach(println)
```
>join 
+ 在类型为（k,v）(k,w)上调用返回一个相同key所有的values都挤在一起-->（k,(v,w)） 的RDD
+ <font color=blue>(k,v)(k,w) ===> (k,(v,w))</font>
##### 初始化
```scala
val rdd = sc.makeRDD(List((1,"桥本环奈"),(2,"木田彩水"), (3,"新恒结衣")))
val rdd2 = sc.makeRDD(List((1,"木田彩水"), (2,"桃乃木"), (3,"石原里美"), (4,"长泽雅美")))
```
##### join
```scala
//join算子相当于内连接仅返回key相同的数据匹配，key匹配不上不关联
val j1 = rdd.join(rdd2)
val j2 = rdd2.join(rdd)
```
##### leftOuterJoin
```scala
//另一张表有数据没办法匹配会自动补全空值
val j3 = rdd2.leftOuterJoin(rdd)
```
+ cogroup-->同一个RDD内先聚合后和其他RDD聚合
```scala
val j4 = rdd2.cogroup(rdd)
```
### Top N
案例 Top3 广告点击
```scala
//读取外部文件创建rdd
val rdd = sc.textFile("F:\\BaiduNetdiskDownload\\大数据\\spark\\input\\agent.log")
 val words = rdd.map(_.split(" ")).map { data => data.toArray }.map(arr=>(arr(1),arr(4)))
val c1 = words.map((s) => (s._1 + "-" + s._2, 1))
val clicks = c1.reduceByKey(_ + _)
//结构转换
val mp = clicks.map {
    //map匹配转换用case
    case (info, c) => {
    val datas = info.split("-")
    (datas(0), (datas(1), c))
    }
}
//按省份分组聚合
val group = mp.groupByKey()
//排序
val sorts = group.mapValues(
    it => {
    it.toList.sortWith {
        (le, ri) => {
        le._2 > ri._2
        }
    }.take(3)
    }
)
```